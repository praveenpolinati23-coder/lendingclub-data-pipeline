# ğŸ’³ LendingClub Data Pipeline â€” End-to-End Data Engineering Project

## ğŸ“˜ Overview

This project demonstrates a **modern, modular data pipeline** built for **LendingClub-like financial data**, designed to showcase **data engineering best practices** â€” ingestion, transformation, validation, and orchestration â€” all running locally in **Docker**.

It integrates:

* **Dagster** â†’ orchestration and observability
* **dbt + DuckDB** â†’ ELT transformations
* **Great Expectations (optional)** â†’ DQ framework (removed for simplicity but extendable)
* **Docker Compose + Makefile** â†’ reproducible runtime environment

The entire pipeline runs end-to-end with **one command** and produces a final output file `account_summary.csv`.

---

## ğŸš€ Features at a Glance

| Stage              | Technology       | Purpose                                              |
| :----------------- | :--------------- | :--------------------------------------------------- |
| **Ingestion**      | Python (Dagster) | Reads raw CSVs (`customers.csv`, `accounts.csv`)     |
| **Transformation** | dbt + DuckDB     | Cleans, joins, and calculates account-level interest |
| **Validation**     | dbt tests        | Column-level DQ checks and thresholds                |
| **Orchestration**  | Dagster          | End-to-end pipeline orchestration & monitoring       |
| **Export**         | DuckDB + Pandas  | Exports final output as `account_summary.csv`        |
| **Runtime**        | Docker           | Fully containerized runtime with Makefile automation |

---

## âš™ï¸ Prerequisites

Before you begin, ensure you have:

* **Docker Desktop** installed and running
* **Git** installed
* **Make** utility available (comes preinstalled on macOS/Linux)

No Python or dbt setup is needed locally â€” everything runs inside Docker.

---

## ğŸ—ï¸ Setup & Execution

### **1ï¸âƒ£ Clone the Repository**

```bash
git clone https://github.com/yourusername/lendingclub-data-pipeline.git
cd lendingclub-data-pipeline
```

---

### **2ï¸âƒ£ Build the Docker Image**

```bash
make build
```

This command:

* Builds the Docker image from `infra/Dockerfile`
* Installs Python, Dagster, dbt, DuckDB, and dependencies
* Prepares internal folders (`dbt/lc_dbt`, `output`, `dagster_home`)

---

### **3ï¸âƒ£ Run the Dagster UI**

```bash
make run
```

Then open your browser at ğŸ‘‰ [http://localhost:3000](http://localhost:3000)

Youâ€™ll see the **Dagster UI**, where you can visualize your entire data pipeline graphically.

---

### **4ï¸âƒ£ View the Output**

After the pipeline completes successfully:

* The final CSV file appears at:

  ```
  output/account_summary.csv
  ```
* You can preview it directly in **Dagster UI â†’ Assets â†’ export_results**
* Or open it from your host machine inside the `output/` folder.

---

## ğŸ§± Folder Structure

```
lendingclub-data-pipeline/
â”‚
â”œâ”€â”€ Makefile                      â†’ Root-level automation (build/run/clean)
â”‚
â”œâ”€â”€ infra/                        â†’ Infrastructure & environment setup
â”‚   â”œâ”€â”€ Dockerfile                â†’ Builds Python + Dagster + dbt + DuckDB image
â”‚   â”œâ”€â”€ docker-compose.yml        â†’ Defines Dagster service & volume mounts
â”‚   â”œâ”€â”€ requirements.txt          â†’ Python dependencies
â”‚
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ lc_dbt/
â”‚       â”œâ”€â”€ data/                 â†’ Raw CSVs (customers.csv, accounts.csv)
â”‚       â”œâ”€â”€ seeds/                â†’ Reference lookups (e.g., interest rates)
â”‚       â”œâ”€â”€ models/               â†’ SQL transformations
â”‚       â”‚   â”œâ”€â”€ staging/          â†’ Cleans raw input data
â”‚       â”‚   â”œâ”€â”€ intermediate/     â†’ Joins, aggregates, computes KPIs
â”‚       â”‚   â””â”€â”€ marts/            â†’ Final models (e.g., fct_interest)
â”‚       â”œâ”€â”€ snapshots/            â†’ Historical tracking (SCD logic)
â”‚       â”œâ”€â”€ dbt_project.yml       â†’ dbt configuration
â”‚       â”œâ”€â”€ profiles.yml          â†’ DuckDB connection profile
â”‚       â”œâ”€â”€ packages.yml          â†’ External dbt utility packages
â”‚       â””â”€â”€ lc_dbt.duckdb         â†’ DuckDB database file (auto-created)
â”‚
â”œâ”€â”€ lc_dagster/                   â†’ Dagster orchestration
â”‚   â”œâ”€â”€ assets/                   â†’ Core pipeline logic
â”‚   â”‚   â”œâ”€â”€ ingest_assets.py      â†’ Reads CSVs and prepares raw inputs
â”‚   â”‚   â”œâ”€â”€ validation_assets.py  â†’ Performs schema & DQ validations
â”‚   â”‚   â”œâ”€â”€ dbt_assets.py         â†’ Executes dbt (run, test, snapshot)
â”‚   â”‚   â””â”€â”€ export_assets.py      â†’ Exports final `account_summary.csv`
â”‚   â”‚
â”‚   â”œâ”€â”€ jobs/                     â†’ Dagster job definitions
â”‚   â”‚   â””â”€â”€ interest_pipeline_job.py â†’ Orchestrates all assets end-to-end
â”‚   â”‚
â”‚   â”œâ”€â”€ constants.py              â†’ Centralized paths (DBT, DuckDB, Output)
â”‚   â””â”€â”€ repository.py             â†’ Registers all assets/jobs in Dagster repo
â”‚
â”œâ”€â”€ output/                       â†’ Stores exported results
â”‚   â””â”€â”€ account_summary.csv       â†’ âœ… Final output CSV file
â”‚
â””â”€â”€ README.md                     â†’ Full documentation (this file)
```

---

## ğŸ”„ Step-by-Step Pipeline Logic

| Step                     | File                   | Function            | Description                                                                     |
| ------------------------ | ---------------------- | ------------------- | ------------------------------------------------------------------------------- |
| **1. Ingest Raw Data**   | `ingest_assets.py`     | `ingest_raw()`      | Reads CSVs from `dbt/lc_dbt/data/` and validates structure                      |
| **2. Validate Raw Data** | `validation_assets.py` | `validate_raw()`    | Checks for missing values, incorrect types, etc.                                |
| **3. Run dbt Models**    | `dbt_assets.py`        | `run_dbt()`         | Executes dbt models (`staging â†’ intermediate â†’ marts`) and tests                |
| **4. Run Snapshots**     | `dbt_assets.py`        | `run_dbt()`         | Maintains history of changing account data                                      |
| **5. Validate Output**   | `validation_assets.py` | `validate_output()` | Ensures business rules and DQ thresholds met                                    |
| **6. Export Results**    | `export_assets.py`     | `export_results()`  | Writes final results to `output/account_summary.csv` and previews in Dagster UI |

---

## DBT Documentation & Lineage Visualization ##

* ğŸ—‚ï¸ Explore Interactive dbt Docs (Data Lineage & Model Documentation)

Once the pipeline is running, you can generate and view dbt documentation inside Docker to explore the full data model lineage, schema, and tests.

ğŸ”§ Steps to Generate and Serve dbt Docs

1ï¸âƒ£ Open a shell inside the running Dagster container

* docker exec -it lendingclub_dagster bash

ğŸ’¡ If the container isnâ€™t running, start it first:

* make run

2ï¸âƒ£ Navigate to the dbt project folder
* cd /app/dbt/lc_dbt
* dbt docs generate --profiles-dir /app/dbt/lc_dbt
* dbt docs serve --profiles-dir /app/dbt/lc_dbt --port 8080 --host 0.0.0.0

* Then open your browser â†’ http://localhost:8080

## ğŸ“Š Key Concepts Covered

| Concept                           | Description                                                      |
| --------------------------------- | ---------------------------------------------------------------- |
| **Modular Orchestration**         | Each asset is independent, promoting testability and reusability |
| **DBT + DuckDB Integration**      | Local, lightweight ELT using SQL                                 |
| **Data Quality Validation**       | dbt tests enforce schema, null, and threshold checks             |
| **Containerized Runtime**         | Docker ensures reproducibility across machines                   |
| **Dagster Metadata**              | Rich UI visualization with logs, lineage, and asset status       |
| **Snapshots & Incremental Logic** | Tracks history of changing data points                           |

---

## ğŸ§  Design Decisions & Trade-offs

| Area                                        | Decision                                                   | Trade-off / Reasoning                            |
| ------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------ |
| **DuckDB**                                  | Chosen over PostgreSQL/Snowflake                           | Lightweight, file-based, ideal for local testing |
| **Dagster over Airflow**                    | Easier setup, great UI, strong local orchestration         | Not as mature for enterprise-scale DAGs          |
| **dbt Tests instead of Great Expectations** | Simpler integration with SQL-based checks                  | Lacks full profiling flexibility                 |
| **Single-thread dbt Execution**             | Ensures deterministic ordering & avoids concurrency issues | Slightly slower runtime                          |
| **Static Input CSVs**                       | Simulates realistic batch data                             | No streaming/real-time data yet                  |


---

## ğŸ§© Future Enhancements

| Category                 | Improvement                                       | Description                                          |
| ------------------------ | ------------------------------------------------- | ---------------------------------------------------- |
| **Data Quality**         | Reintroduce **Great Expectations**                | Richer validation & data scoring with thresholds     |
| **Scheduling**           | Add **Dagster sensors/schedules**                 | Automate daily or hourly pipeline runs               |
| **Machine Learning**     | Integrate a model for **loan default prediction** | Train ML model using intermediate tables as features |
| **Lineage & Governance** | Add **Atlan or OpenMetadata integration**         | Track dataset lineage and ownership automatically    |
| **Monitoring**           | Add **Prometheus + Grafana**                      | Dashboard for run history, latency, and DQ metrics   |
| **Cloud Migration**      | Move from DuckDB â†’ Snowflake/BigQuery             | Enable scalability & multi-user collaboration        |
| **CI/CD**                | Add GitHub Actions workflow                       | Auto-build & test dbt/Dagster changes on PRs         |
| **Delta Snapshots**      | Incremental snapshotting                          | Store deltas instead of overwriting full data        |
| **Dynamic Inputs**       | S3 or API-based ingestion                         | Replace static CSVs with real upstream systems       |


---

## ğŸ’¡ Why This Matters

This project mimics a **real-world data engineering pipeline** in a **reproducible local environment**, showing:

* Clear data lineage
* Testable transformations
* Automated orchestration
* Extensible architecture for ML and cloud migration
